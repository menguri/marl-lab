[QMIX]
- run_with_wandb.py에서 arg.parameters를 lower()하는 바람에 reward_scale=True로 잡혀있었음.
기존의 reward에 20배 곱하여 sparse를 차단하는 것을 smac에 적용하고 있어 성능이 낮았던 것으로 보임. > reward_scale 킬거면 only_positive나 scale_rate 조정 필요.
* VDN과 IQL에선, IQL은 단순히 자기가 이겼을 때/졌을 때로 reward를 직접 받고, VDN은 단순 합이기 때문에 목표 값이 나아지는 것이지 학습 망가지진 않음.
* QMIX는 reward_scale, only_positive, stardardis_rewards 모두 걸어버리면 reward가 너무 평탄해져서 학습되지 않음.